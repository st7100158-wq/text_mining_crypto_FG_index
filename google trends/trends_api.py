import pandas as pd
import time
import random
from pytrends.request import TrendReq
from pytrends.exceptions import TooManyRequestsError
from sklearn.preprocessing import MinMaxScaler

# === åˆå§‹åŒ– pytrends ===
pytrends = TrendReq(
    hl='en-US',
    tz=360,
    geo='US',
    retries=8,
    backoff_factor=1.0,
    timeout=(10, 60),
    requests_args={
        "headers": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
        }
    },
)

# === å®‰å…¨æŸ¥è©¢å‡½å¼ ===
def safe_interest(pytrends, keywords, timeframe):
    backoff_seconds = 120
    max_backoff = 600
    while True:
        try:
            time.sleep(random.uniform(3, 8))
            pytrends.build_payload(keywords, timeframe=timeframe)
            data = pytrends.interest_over_time()
            if not data.empty:
                print(f"âœ… æˆåŠŸæŠ“å–: {keywords}")
                return data
        except TooManyRequestsError:
            wait_for = min(max_backoff, backoff_seconds) + random.uniform(5, 25)
            print(f"ğŸš« 429 Too many requests. Waiting {int(wait_for)}s...")
            time.sleep(wait_for)
            backoff_seconds = min(max_backoff, backoff_seconds * 2)
        except Exception as e:
            wait_for = min(max_backoff, backoff_seconds) + random.uniform(5, 15)
            print(f"âš ï¸ Error: {e}, waiting {int(wait_for)}s and retrying...")
            time.sleep(wait_for)
            backoff_seconds = min(max_backoff, backoff_seconds * 2)

# === æ¨™æº–åŒ–å‡½å¼ ===
def normalize_keywords(data):
    """å°æ¯å€‹é—œéµå­—åˆ†åˆ¥é€²è¡Œæ¨™æº–åŒ– (0-100)"""
    scaler = MinMaxScaler()
    normalized_data = data.copy()
    
    for col in data.columns:
        if col != 'isPartial':
            # å°‡æ¯å€‹é—œéµå­—çš„æ•¸æ“šæ¨™æº–åŒ–åˆ° 0-100 ç¯„åœ
            normalized_data[col] = scaler.fit_transform(data[[col]].values) * 100
    
    return normalized_data

# === é—œéµå­—è¨­å®š ===
fear_keywords = ["Ethereum crash","Ethereum fraud","Ethereum sell","Ethereum risk"]
greed_keywords = ["buy Ethereum","Ethereum profit","Ethereum bull run"]
fear_keywords2 = ["Ethereum fear","Ethereum scam","Ethereum bubble", "SEC ETH"]
greed_keywords2= ["Ethereum investment", "the most profitable crypto","Ethereum DeFi","Ethereum surge", "Ethereum growth"]
print("é–‹å§‹æŠ“å–ææ‡¼é—œéµå­—...")
fear_data = safe_interest(pytrends, fear_keywords, '2024-10-01 2025-10-01')

time.sleep(random.uniform(45, 90))
fear_data2 = safe_interest(pytrends, fear_keywords2, '2024-10-01 2025-10-01')

print("é–‹å§‹æŠ“å–è²ªå©ªé—œéµå­—...")
time.sleep(random.uniform(45, 90))
greed_data = safe_interest(pytrends, greed_keywords, '2024-10-01 2025-10-01')
time.sleep(random.uniform(45, 90))
greed_data2 = safe_interest(pytrends, greed_keywords2, '2024-10-01 2025-10-01')
# === è³‡æ–™æ¸…ç†èˆ‡æ¨™æº–åŒ– ===

fear_data = fear_data.drop(columns=['isPartial'])
fear_data2 = fear_data2.drop(columns=['isPartial'])
fear_data = pd.merge(fear_data2, fear_data, on='date', how='left')
greed_data = greed_data.drop(columns=['isPartial'])
greed_data2 = greed_data2.drop(columns=['isPartial'])
greed_data = pd.merge(greed_data2, greed_data, on='date', how='left')
print("\n=== æ¨™æº–åŒ–å‰çµ±è¨ˆ ===")
print("ææ‡¼é—œéµå­—å¹³å‡å€¼:")
print(fear_data.mean().round(2))
print("\nè²ªå©ªé—œéµå­—å¹³å‡å€¼:")
print(greed_data.mean().round(2))

# å°æ¯å€‹é—œéµå­—é€²è¡Œæ¨™æº–åŒ–
fear_normalized = normalize_keywords(fear_data)
greed_normalized = normalize_keywords(greed_data)

print("\n=== æ¨™æº–åŒ–å¾Œçµ±è¨ˆ ===")
print("æ¨™æº–åŒ–ææ‡¼é—œéµå­—å¹³å‡å€¼:")
print(fear_normalized.mean().round(2))
print("\næ¨™æº–åŒ–è²ªå©ªé—œéµå­—å¹³å‡å€¼:")
print(greed_normalized.mean().round(2))

# === è¨ˆç®—æƒ…ç·’æŒ‡æ•¸ ===
fear_index = fear_normalized.mean(axis=1)
greed_index = greed_normalized.mean(axis=1)

emotion_index = pd.DataFrame({
    'Fear_Index': fear_index,
    'Greed_Index': greed_index,
})
emotion_index['Greed_Fear_Score'] = emotion_index['Greed_Index'] - emotion_index['Fear_Index']

# å°‡ç¶œåˆåˆ†æ•¸ä¹Ÿæ¨™æº–åŒ–åˆ° 0-100 ç¯„åœä»¥ä¾¿è§£è®€
scaler_final = MinMaxScaler()
emotion_index['Greed_Fear_Normalized'] = scaler_final.fit_transform(
    emotion_index[['Greed_Fear_Score']].values
) * 100

# === å„²å­˜èˆ‡é¡¯ç¤º ===
emotion_index.to_csv('emotion_index_normalized.csv', encoding='utf-8-sig')
print("\nğŸ’¾ å·²å„²å­˜æˆ emotion_index_normalized.csv")
print("\nå‰5å¤©æ•¸æ“š:")
print(emotion_index.head())
